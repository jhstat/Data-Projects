{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI SECOM Semiconductor Defect Detection Analysis\n",
    "\n",
    "**Objective**: Predict semiconductor manufacturing failures (defects) using sensor data.\n",
    "\n",
    "**Approach**:\n",
    "1. Label-free feature engineering (to avoid data leakage)\n",
    "2. Multiple resampling strategies for class imbalance\n",
    "3. Multiple models with hyperparameter tuning via CV\n",
    "4. Primary metric: Recall@Precision>0.2\n",
    "5. SHAP interpretability for top performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Install required libraries (run once)\n",
    "# Uncomment and run if any packages are missing\n",
    "\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn lightgbm xgboost imbalanced-learn shap --quiet\n",
    "\n",
    "print(\"All required packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_recall_curve, auc,\n",
    "    confusion_matrix, classification_report,\n",
    "    average_precision_score, recall_score, precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import shap\n",
    "import warnings\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Data\n",
    "df = pd.read_csv('uci-secom.csv')\n",
    "\n",
    "# Rename target column\n",
    "df = df.rename(columns={'Pass/Fail': 'target'})\n",
    "\n",
    "# Convert target: -1 (Pass) -> 0, 1 (Fail) -> 1\n",
    "df['target'] = df['target'].map({-1: 0, 1: 1})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nDefect rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Train-Test Split (80/20 stratified)\n",
    "\n",
    "# Separate features and target\n",
    "# Drop 'Time' column if it exists (not a sensor feature)\n",
    "feature_cols = [col for col in df.columns if col not in ['target', 'Time']]\n",
    "X = df[feature_cols]\n",
    "y = df['target']\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTrain defect rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test defect rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (Label-Free)\n",
    "\n",
    "All feature selection is done without looking at labels to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Remove columns with >=80% missing values\n",
    "\n",
    "# Calculate missingness on training set\n",
    "missing_pct = X_train.isnull().mean() * 100\n",
    "\n",
    "# Columns to drop (>=80% missing)\n",
    "cols_drop_high_missing = missing_pct[missing_pct >= 80].index.tolist()\n",
    "\n",
    "print(f\"Columns with >=80% missing: {len(cols_drop_high_missing)}\")\n",
    "print(f\"Examples: {cols_drop_high_missing[:5]}\")\n",
    "\n",
    "# Store for later\n",
    "feature_engineering_log = {'original_features': len(feature_cols)}\n",
    "feature_engineering_log['dropped_high_missing'] = len(cols_drop_high_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create missingness indicators for 50-80% missing columns\n",
    "\n",
    "# Columns with 50-80% missing\n",
    "cols_medium_missing = missing_pct[(missing_pct >= 50) & (missing_pct < 80)].index.tolist()\n",
    "\n",
    "print(f\"Columns with 50-80% missing: {len(cols_medium_missing)}\")\n",
    "\n",
    "# Create missingness indicator column names\n",
    "missingness_indicators = [f\"{col}_missing\" for col in cols_medium_missing]\n",
    "\n",
    "feature_engineering_log['medium_missing_to_indicators'] = len(cols_medium_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Remove constant and near-constant columns\n",
    "\n",
    "# Work with remaining columns (exclude high missing ones)\n",
    "remaining_cols = [col for col in X_train.columns \n",
    "                  if col not in cols_drop_high_missing and col not in cols_medium_missing]\n",
    "\n",
    "cols_to_drop_constant = []\n",
    "\n",
    "for col in remaining_cols:\n",
    "    col_data = X_train[col].dropna()\n",
    "    \n",
    "    if len(col_data) == 0:\n",
    "        cols_to_drop_constant.append(col)\n",
    "        continue\n",
    "    \n",
    "    # Check 1: Single unique value\n",
    "    if col_data.nunique() == 1:\n",
    "        cols_to_drop_constant.append(col)\n",
    "        continue\n",
    "    \n",
    "    # Check 2: IQR = 0 (zero variability in middle 50%)\n",
    "    q1, q3 = col_data.quantile([0.25, 0.75])\n",
    "    if q3 - q1 == 0:\n",
    "        cols_to_drop_constant.append(col)\n",
    "        continue\n",
    "    \n",
    "    # Check 3: Top value >99% frequency\n",
    "    top_freq = col_data.value_counts(normalize=True).iloc[0]\n",
    "    if top_freq > 0.99:\n",
    "        cols_to_drop_constant.append(col)\n",
    "        continue\n",
    "    \n",
    "    # Check 4: count_nonmode < 20\n",
    "    mode_val = col_data.mode().iloc[0] if len(col_data.mode()) > 0 else None\n",
    "    if mode_val is not None:\n",
    "        count_nonmode = (col_data != mode_val).sum()\n",
    "        if count_nonmode < 20:\n",
    "            cols_to_drop_constant.append(col)\n",
    "            continue\n",
    "\n",
    "print(f\"Columns dropped (constant/near-constant): {len(cols_to_drop_constant)}\")\n",
    "\n",
    "feature_engineering_log['dropped_constant'] = len(cols_to_drop_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Remove highly correlated columns (>95%)\n",
    "\n",
    "# Get columns remaining after previous steps\n",
    "cols_for_corr = [col for col in remaining_cols if col not in cols_to_drop_constant]\n",
    "\n",
    "print(f\"Checking correlation among {len(cols_for_corr)} columns...\")\n",
    "\n",
    "# Calculate correlation matrix (use pairwise complete observations)\n",
    "corr_matrix = X_train[cols_for_corr].corr().abs()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "cols_to_drop_corr = set()\n",
    "checked_pairs = set()\n",
    "\n",
    "for i, col1 in enumerate(cols_for_corr):\n",
    "    for col2 in cols_for_corr[i+1:]:\n",
    "        if (col1, col2) in checked_pairs or col1 in cols_to_drop_corr or col2 in cols_to_drop_corr:\n",
    "            continue\n",
    "        \n",
    "        corr_val = corr_matrix.loc[col1, col2]\n",
    "        \n",
    "        if corr_val > 0.95:\n",
    "            # Decide which to drop based on:\n",
    "            # 1. More missing values\n",
    "            # 2. Lower variability (std)\n",
    "            missing1 = X_train[col1].isnull().sum()\n",
    "            missing2 = X_train[col2].isnull().sum()\n",
    "            \n",
    "            if missing1 != missing2:\n",
    "                drop_col = col1 if missing1 > missing2 else col2\n",
    "            else:\n",
    "                std1 = X_train[col1].std()\n",
    "                std2 = X_train[col2].std()\n",
    "                drop_col = col1 if std1 < std2 else col2\n",
    "            \n",
    "            cols_to_drop_corr.add(drop_col)\n",
    "        \n",
    "        checked_pairs.add((col1, col2))\n",
    "\n",
    "cols_to_drop_corr = list(cols_to_drop_corr)\n",
    "print(f\"Columns dropped (high correlation): {len(cols_to_drop_corr)}\")\n",
    "\n",
    "feature_engineering_log['dropped_correlated'] = len(cols_to_drop_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Lock feature set\n",
    "\n",
    "# Final feature columns (original features to keep)\n",
    "all_cols_to_drop = set(cols_drop_high_missing + cols_medium_missing + \n",
    "                       cols_to_drop_constant + cols_to_drop_corr)\n",
    "\n",
    "final_feature_cols = [col for col in feature_cols if col not in all_cols_to_drop]\n",
    "\n",
    "# Add missingness indicators to final feature list\n",
    "final_feature_cols_with_indicators = final_feature_cols + missingness_indicators\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original features: {feature_engineering_log['original_features']}\")\n",
    "print(f\"Dropped (>=80% missing): {feature_engineering_log['dropped_high_missing']}\")\n",
    "print(f\"Converted to indicators (50-80% missing): {feature_engineering_log['medium_missing_to_indicators']}\")\n",
    "print(f\"Dropped (constant/near-constant): {feature_engineering_log['dropped_constant']}\")\n",
    "print(f\"Dropped (high correlation): {feature_engineering_log['dropped_correlated']}\")\n",
    "print(f\"\\nFinal numeric features: {len(final_feature_cols)}\")\n",
    "print(f\"Missingness indicators: {len(missingness_indicators)}\")\n",
    "print(f\"Total final features: {len(final_feature_cols_with_indicators)}\")\n",
    "\n",
    "# Store for later use\n",
    "FINAL_NUMERIC_COLS = final_feature_cols\n",
    "MISSINGNESS_INDICATOR_SOURCE_COLS = cols_medium_missing\n",
    "MISSINGNESS_INDICATOR_COLS = missingness_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Define CV Splits\n",
    "\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Using {N_SPLITS}-fold stratified cross-validation\")\n",
    "\n",
    "# Preview fold sizes\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"Fold {fold_idx+1}: Train={len(train_idx)}, Val={len(val_idx)}, \"\n",
    "          f\"Val defect rate={y_train.iloc[val_idx].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Data preparation function\n",
    "\n",
    "def prepare_features(X_data, numeric_cols, indicator_source_cols, indicator_cols):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix by:\n",
    "    1. Selecting numeric columns\n",
    "    2. Creating missingness indicators\n",
    "    \"\"\"\n",
    "    # Select numeric features\n",
    "    X_numeric = X_data[numeric_cols].copy()\n",
    "    \n",
    "    # Create missingness indicators\n",
    "    for src_col, ind_col in zip(indicator_source_cols, indicator_cols):\n",
    "        if src_col in X_data.columns:\n",
    "            X_numeric[ind_col] = X_data[src_col].isnull().astype(int)\n",
    "        else:\n",
    "            X_numeric[ind_col] = 0\n",
    "    \n",
    "    return X_numeric\n",
    "\n",
    "print(\"prepare_features() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Imputation and scaling functions\n",
    "\n",
    "def impute_and_scale(X_train_fold, X_val_fold):\n",
    "    \"\"\"\n",
    "    Impute missing values with median and scale features.\n",
    "    Fitted on training fold only, applied to both.\n",
    "    \"\"\"\n",
    "    # Imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train_fold),\n",
    "        columns=X_train_fold.columns,\n",
    "        index=X_train_fold.index\n",
    "    )\n",
    "    X_val_imputed = pd.DataFrame(\n",
    "        imputer.transform(X_val_fold),\n",
    "        columns=X_val_fold.columns,\n",
    "        index=X_val_fold.index\n",
    "    )\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train_imputed),\n",
    "        columns=X_train_imputed.columns,\n",
    "        index=X_train_imputed.index\n",
    "    )\n",
    "    X_val_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_val_imputed),\n",
    "        columns=X_val_imputed.columns,\n",
    "        index=X_val_imputed.index\n",
    "    )\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, imputer, scaler\n",
    "\n",
    "print(\"impute_and_scale() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Resampling strategies\n",
    "\n",
    "def apply_resampling(X_train_fold, y_train_fold, strategy):\n",
    "    \"\"\"\n",
    "    Apply resampling strategy to training data.\n",
    "    \n",
    "    Strategies:\n",
    "    - 'none': No resampling (use class weights in model)\n",
    "    - 'undersample': Random undersampling to 1:3 pos:neg ratio\n",
    "    - 'smote_tomek': SMOTE + Tomek links\n",
    "    - 'smote_enn': SMOTE + ENN (k=3)\n",
    "    \"\"\"\n",
    "    if strategy == 'none':\n",
    "        return X_train_fold, y_train_fold\n",
    "    \n",
    "    elif strategy == 'undersample':\n",
    "        # Target 1:3 ratio (pos:neg)\n",
    "        n_pos = y_train_fold.sum()\n",
    "        n_neg_target = n_pos * 3\n",
    "        \n",
    "        rus = RandomUnderSampler(\n",
    "            sampling_strategy={0: min(n_neg_target, (y_train_fold == 0).sum()), 1: n_pos},\n",
    "            random_state=42\n",
    "        )\n",
    "        X_resampled, y_resampled = rus.fit_resample(X_train_fold, y_train_fold)\n",
    "        return X_resampled, y_resampled\n",
    "    \n",
    "    elif strategy == 'smote_tomek':\n",
    "        # SMOTE first, then Tomek links\n",
    "        n_pos = y_train_fold.sum()\n",
    "        k_neighbors = min(3, n_pos - 1) if n_pos > 1 else 1\n",
    "        \n",
    "        smote_tomek = SMOTETomek(\n",
    "            smote=SMOTE(k_neighbors=k_neighbors, random_state=42),\n",
    "            random_state=42\n",
    "        )\n",
    "        X_resampled, y_resampled = smote_tomek.fit_resample(X_train_fold, y_train_fold)\n",
    "        return X_resampled, y_resampled\n",
    "    \n",
    "    elif strategy == 'smote_enn':\n",
    "        # SMOTE + ENN with k=3\n",
    "        n_pos = y_train_fold.sum()\n",
    "        k_neighbors = min(3, n_pos - 1) if n_pos > 1 else 1\n",
    "        \n",
    "        smote_enn = SMOTEENN(\n",
    "            smote=SMOTE(k_neighbors=k_neighbors, random_state=42),\n",
    "            random_state=42\n",
    "        )\n",
    "        X_resampled, y_resampled = smote_enn.fit_resample(X_train_fold, y_train_fold)\n",
    "        return X_resampled, y_resampled\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown resampling strategy: {strategy}\")\n",
    "\n",
    "RESAMPLING_STRATEGIES = ['none', 'undersample', 'smote_tomek', 'smote_enn']\n",
    "print(f\"Resampling strategies: {RESAMPLING_STRATEGIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 13: Define model configurations with hyperparameter grids\n\nMODEL_CONFIGS = {\n    'LogisticRegression': {\n        'model_class': LogisticRegression,\n        'base_params': {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 1000, 'random_state': 42},\n        'param_grid': {\n            'C': [0.01, 0.1, 1.0],\n            'l1_ratio': [0.3, 0.5, 0.7]\n        }\n    },\n    'LightGBM': {\n        'model_class': LGBMClassifier,\n        'base_params': {'random_state': 42, 'verbose': -1, 'n_jobs': -1},\n        'param_grid': {\n            'num_leaves': [15, 31],\n            'min_data_in_leaf': [10, 20],\n            'learning_rate': [0.05, 0.1]\n        }\n    },\n    'XGBoost': {\n        'model_class': XGBClassifier,\n        'base_params': {'random_state': 42, 'eval_metric': 'logloss', 'n_jobs': -1},\n        'param_grid': {\n            'max_depth': [3, 5],\n            'min_child_weight': [1, 5],\n            'learning_rate': [0.05, 0.1]  # XGBoost uses learning_rate, not eta\n        }\n    },\n    'RandomForest': {\n        'model_class': RandomForestClassifier,\n        'base_params': {'random_state': 42, 'n_jobs': -1},\n        'param_grid': {\n            'n_estimators': [100, 200],\n            'max_depth': [5, 10],\n            'min_samples_leaf': [5, 10]\n        }\n    }\n}\n\n# Display grid sizes\nfor model_name, config in MODEL_CONFIGS.items():\n    grid_size = 1\n    for param_values in config['param_grid'].values():\n        grid_size *= len(param_values)\n    print(f\"{model_name}: {grid_size} hyperparameter combinations\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 14: Evaluation metrics\n\ndef recall_at_precision_threshold(y_true, y_proba, precision_threshold=0.2):\n    \"\"\"\n    Calculate recall at the threshold where precision >= precision_threshold.\n    Returns the maximum recall achievable while maintaining precision >= threshold.\n    Used ONLY during CV for threshold optimization.\n    \"\"\"\n    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)\n    \n    # Find thresholds where precision >= threshold\n    valid_indices = np.where(precisions >= precision_threshold)[0]\n    \n    if len(valid_indices) == 0:\n        return 0.0, None  # No threshold achieves required precision\n    \n    # Get maximum recall among valid thresholds\n    max_recall_idx = valid_indices[np.argmax(recalls[valid_indices])]\n    best_recall = recalls[max_recall_idx]\n    \n    # Get corresponding threshold (handle edge case)\n    if max_recall_idx < len(thresholds):\n        best_threshold = thresholds[max_recall_idx]\n    else:\n        best_threshold = thresholds[-1] if len(thresholds) > 0 else 0.5\n    \n    return best_recall, best_threshold\n\n\ndef find_optimal_threshold(y_true, y_proba, precision_threshold=0.2):\n    \"\"\"\n    Find optimal threshold that maximizes recall while maintaining precision >= threshold.\n    Fallback: if no threshold satisfies precision constraint, use max F1.\n    \"\"\"\n    recall_at_prec, threshold = recall_at_precision_threshold(y_true, y_proba, precision_threshold)\n    \n    if threshold is not None and recall_at_prec > 0:\n        return threshold, 'recall@precision'\n    \n    # Fallback: find threshold with max F1\n    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)\n    \n    # Calculate F1 for each threshold\n    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n    \n    best_f1_idx = np.argmax(f1_scores[:-1])  # Exclude last element (undefined threshold)\n    best_threshold = thresholds[best_f1_idx]\n    \n    return best_threshold, 'max_f1'\n\n\ndef calculate_metrics_cv(y_true, y_proba, threshold=0.5):\n    \"\"\"\n    Calculate metrics for CV (includes threshold sweeping for recall@prec>0.2).\n    \"\"\"\n    y_pred = (y_proba >= threshold).astype(int)\n    \n    pr_auc = average_precision_score(y_true, y_proba)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall_at_prec, _ = recall_at_precision_threshold(y_true, y_proba, 0.2)\n    \n    return {\n        'pr_auc': pr_auc,\n        'f1': f1,\n        'recall_at_prec_0.2': recall_at_prec,\n        'recall': recall,\n        'precision': precision,\n        'threshold': threshold\n    }\n\n\ndef calculate_metrics_test(y_true, y_proba, threshold):\n    \"\"\"\n    Calculate metrics for test set (fixed threshold from CV, no sweeping).\n    \"\"\"\n    y_pred = (y_proba >= threshold).astype(int)\n    \n    pr_auc = average_precision_score(y_true, y_proba)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    \n    return {\n        'pr_auc': pr_auc,\n        'f1': f1,\n        'recall': recall,\n        'precision': precision,\n        'threshold': threshold\n    }\n\nprint(\"Evaluation functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Main CV training loop\n",
    "\n",
    "def run_cv_experiment(model_name, model_config, resampling_strategy, X_train_full, y_train_full, skf):\n",
    "    \"\"\"\n",
    "    Run cross-validation for a single model + resampling combination.\n",
    "    Returns OOF predictions and best hyperparameters.\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    X_train_prepared = prepare_features(\n",
    "        X_train_full, FINAL_NUMERIC_COLS, \n",
    "        MISSINGNESS_INDICATOR_SOURCE_COLS, MISSINGNESS_INDICATOR_COLS\n",
    "    )\n",
    "    \n",
    "    # Generate all hyperparameter combinations\n",
    "    param_grid = model_config['param_grid']\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    best_oof_proba = None\n",
    "    \n",
    "    # Grid search\n",
    "    for param_values in param_combinations:\n",
    "        params = dict(zip(param_names, param_values))\n",
    "        full_params = {**model_config['base_params'], **params}\n",
    "        \n",
    "        # Add class_weight for 'none' resampling strategy\n",
    "        if resampling_strategy == 'none' and model_name != 'XGBoost':\n",
    "            full_params['class_weight'] = 'balanced'\n",
    "        elif resampling_strategy == 'none' and model_name == 'XGBoost':\n",
    "            # XGBoost uses scale_pos_weight\n",
    "            n_neg = (y_train_full == 0).sum()\n",
    "            n_pos = (y_train_full == 1).sum()\n",
    "            full_params['scale_pos_weight'] = n_neg / n_pos\n",
    "        \n",
    "        # Collect OOF predictions\n",
    "        oof_proba = np.zeros(len(y_train_full))\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train_prepared, y_train_full)):\n",
    "            # Split\n",
    "            X_fold_train = X_train_prepared.iloc[train_idx]\n",
    "            X_fold_val = X_train_prepared.iloc[val_idx]\n",
    "            y_fold_train = y_train_full.iloc[train_idx]\n",
    "            y_fold_val = y_train_full.iloc[val_idx]\n",
    "            \n",
    "            # Impute and scale\n",
    "            X_fold_train_scaled, X_fold_val_scaled, _, _ = impute_and_scale(X_fold_train, X_fold_val)\n",
    "            \n",
    "            # Resample (training only)\n",
    "            X_fold_train_resampled, y_fold_train_resampled = apply_resampling(\n",
    "                X_fold_train_scaled, y_fold_train, resampling_strategy\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            model = model_config['model_class'](**full_params)\n",
    "            model.fit(X_fold_train_resampled, y_fold_train_resampled)\n",
    "            \n",
    "            # Predict probabilities\n",
    "            fold_proba = model.predict_proba(X_fold_val_scaled)[:, 1]\n",
    "            oof_proba[val_idx] = fold_proba\n",
    "            \n",
    "            # Fold score (PR-AUC for selection)\n",
    "            fold_score = average_precision_score(y_fold_val, fold_proba)\n",
    "            fold_scores.append(fold_score)\n",
    "        \n",
    "        # Average score across folds\n",
    "        mean_score = np.mean(fold_scores)\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "            best_oof_proba = oof_proba.copy()\n",
    "    \n",
    "    return best_oof_proba, best_params, best_score\n",
    "\n",
    "print(\"run_cv_experiment() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 16: Run all experiments\n\nprint(\"=\"*60)\nprint(\"STARTING CROSS-VALIDATION EXPERIMENTS\")\nprint(\"=\"*60)\n\nresults = []\noof_predictions = {}  # Store OOF predictions for threshold optimization\n\ntotal_experiments = len(MODEL_CONFIGS) * len(RESAMPLING_STRATEGIES)\nexp_count = 0\n\nfor model_name, model_config in MODEL_CONFIGS.items():\n    for resampling in RESAMPLING_STRATEGIES:\n        exp_count += 1\n        print(f\"\\n[{exp_count}/{total_experiments}] {model_name} + {resampling}\")\n        \n        try:\n            # Run CV\n            oof_proba, best_params, cv_pr_auc = run_cv_experiment(\n                model_name, model_config, resampling, X_train, y_train, skf\n            )\n            \n            # Find optimal threshold\n            opt_threshold, threshold_method = find_optimal_threshold(y_train, oof_proba, 0.2)\n            \n            # Calculate metrics with optimal threshold (CV uses sweep)\n            metrics = calculate_metrics_cv(y_train, oof_proba, opt_threshold)\n            \n            # Store results\n            result = {\n                'model': model_name,\n                'resampling': resampling,\n                'best_params': best_params,\n                'cv_pr_auc': cv_pr_auc,\n                'optimal_threshold': opt_threshold,\n                'threshold_method': threshold_method,\n                **metrics\n            }\n            results.append(result)\n            \n            # Store OOF predictions\n            key = f\"{model_name}_{resampling}\"\n            oof_predictions[key] = {\n                'proba': oof_proba,\n                'best_params': best_params,\n                'threshold': opt_threshold\n            }\n            \n            print(f\"  Best params: {best_params}\")\n            print(f\"  CV PR-AUC: {cv_pr_auc:.4f}\")\n            print(f\"  Recall@Prec>0.2: {metrics['recall_at_prec_0.2']:.4f}\")\n            print(f\"  F1: {metrics['f1']:.4f}\")\n            print(f\"  Optimal threshold: {opt_threshold:.3f} ({threshold_method})\")\n            \n        except Exception as e:\n            print(f\"  ERROR: {str(e)}\")\n            continue\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CV EXPERIMENTS COMPLETE\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Results summary\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by primary metric (Recall@Precision>0.2)\n",
    "results_df = results_df.sort_values('recall_at_prec_0.2', ascending=False)\n",
    "\n",
    "# Display key columns\n",
    "display_cols = ['model', 'resampling', 'recall_at_prec_0.2', 'f1', 'pr_auc', 'optimal_threshold']\n",
    "print(\"\\nCV RESULTS (sorted by Recall@Precision>0.2):\")\n",
    "print(results_df[display_cols].to_string(index=False))\n",
    "\n",
    "# Best performers\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 3 PERFORMERS:\")\n",
    "print(\"=\"*60)\n",
    "for i, row in results_df.head(3).iterrows():\n",
    "    print(f\"\\n{row['model']} + {row['resampling']}\")\n",
    "    print(f\"  Recall@Prec>0.2: {row['recall_at_prec_0.2']:.4f}\")\n",
    "    print(f\"  F1: {row['f1']:.4f}\")\n",
    "    print(f\"  PR-AUC: {row['pr_auc']:.4f}\")\n",
    "    print(f\"  Best params: {row['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Prepare full training and test sets\n",
    "\n",
    "# Prepare features for full training set\n",
    "X_train_prepared = prepare_features(\n",
    "    X_train, FINAL_NUMERIC_COLS,\n",
    "    MISSINGNESS_INDICATOR_SOURCE_COLS, MISSINGNESS_INDICATOR_COLS\n",
    ")\n",
    "\n",
    "# Prepare features for test set\n",
    "X_test_prepared = prepare_features(\n",
    "    X_test, FINAL_NUMERIC_COLS,\n",
    "    MISSINGNESS_INDICATOR_SOURCE_COLS, MISSINGNESS_INDICATOR_COLS\n",
    ")\n",
    "\n",
    "# Impute and scale (fit on full training set)\n",
    "X_train_final, X_test_final, final_imputer, final_scaler = impute_and_scale(\n",
    "    X_train_prepared, X_test_prepared\n",
    ")\n",
    "\n",
    "print(f\"Final training set shape: {X_train_final.shape}\")\n",
    "print(f\"Final test set shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 19: Refit top models and evaluate on test set\n\nprint(\"=\"*60)\nprint(\"TEST SET EVALUATION\")\nprint(\"=\"*60)\n\ntest_results = []\nfinal_models = {}  # Store for SHAP\n\n# Evaluate all model + resampling combinations\nfor idx, row in results_df.iterrows():\n    model_name = row['model']\n    resampling = row['resampling']\n    best_params = row['best_params']\n    opt_threshold = row['optimal_threshold']\n    \n    model_config = MODEL_CONFIGS[model_name]\n    \n    print(f\"\\nEvaluating: {model_name} + {resampling}\")\n    \n    try:\n        # Prepare params\n        full_params = {**model_config['base_params'], **best_params}\n        \n        if resampling == 'none' and model_name != 'XGBoost':\n            full_params['class_weight'] = 'balanced'\n        elif resampling == 'none' and model_name == 'XGBoost':\n            n_neg = (y_train == 0).sum()\n            n_pos = (y_train == 1).sum()\n            full_params['scale_pos_weight'] = n_neg / n_pos\n        \n        # Apply resampling to full training set\n        X_train_resampled, y_train_resampled = apply_resampling(\n            X_train_final, y_train, resampling\n        )\n        \n        # Fit model\n        model = model_config['model_class'](**full_params)\n        model.fit(X_train_resampled, y_train_resampled)\n        \n        # Predict on test set\n        test_proba = model.predict_proba(X_test_final)[:, 1]\n        test_pred = (test_proba >= opt_threshold).astype(int)\n        \n        # Calculate metrics - use fixed threshold, no sweeping\n        test_metrics = calculate_metrics_test(y_test, test_proba, opt_threshold)\n        \n        test_result = {\n            'model': model_name,\n            'resampling': resampling,\n            **test_metrics\n        }\n        test_results.append(test_result)\n        \n        # Store model\n        key = f\"{model_name}_{resampling}\"\n        final_models[key] = {\n            'model': model,\n            'threshold': opt_threshold,\n            'test_proba': test_proba,\n            'test_pred': test_pred\n        }\n        \n        print(f\"  F1: {test_metrics['f1']:.4f}\")\n        print(f\"  PR-AUC: {test_metrics['pr_auc']:.4f}\")\n        print(f\"  Recall: {test_metrics['recall']:.4f}\")\n        print(f\"  Precision: {test_metrics['precision']:.4f}\")\n        \n    except Exception as e:\n        print(f\"  ERROR: {str(e)}\")\n        continue\n\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 20: Test results summary and ranking\n\ntest_results_df = pd.DataFrame(test_results)\ntest_results_df = test_results_df.sort_values('f1', ascending=False)\n\nprint(\"\\nTEST SET RESULTS (sorted by F1):\")\nprint(\"=\"*80)\ndisplay_cols = ['model', 'resampling', 'f1', 'pr_auc', 'recall', 'precision', 'threshold']\nprint(test_results_df[display_cols].to_string(index=False))\n\n# Highlight top performers\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOP 3 TEST SET PERFORMERS (by F1):\")\nprint(\"=\"*80)\ntop_3 = test_results_df.head(3)\nfor i, (_, row) in enumerate(top_3.iterrows()):\n    print(f\"\\n#{i+1}: {row['model']} + {row['resampling']}\")\n    print(f\"     F1: {row['f1']:.4f}\")\n    print(f\"     PR-AUC: {row['pr_auc']:.4f}\")\n    print(f\"     Recall: {row['recall']:.4f}, Precision: {row['precision']:.4f}\")\n    print(f\"     Threshold (from CV): {row['threshold']:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Confusion matrices for top 3\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "top_3_keys = [f\"{row['model']}_{row['resampling']}\" for _, row in top_3.iterrows()]\n",
    "\n",
    "for ax, key in zip(axes, top_3_keys):\n",
    "    if key in final_models:\n",
    "        test_pred = final_models[key]['test_pred']\n",
    "        cm = confusion_matrix(y_test, test_pred)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                    xticklabels=['Pass', 'Fail'], yticklabels=['Pass', 'Fail'])\n",
    "        ax.set_title(key.replace('_', '\\n'))\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Confusion Matrices - Top 3 Models', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: PR curves for top 3\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for key in top_3_keys:\n",
    "    if key in final_models:\n",
    "        test_proba = final_models[key]['test_proba']\n",
    "        precision, recall, _ = precision_recall_curve(y_test, test_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        plt.plot(recall, precision, label=f\"{key} (AUC={pr_auc:.3f})\")\n",
    "\n",
    "plt.axhline(y=0.2, color='r', linestyle='--', label='Precision=0.2 threshold')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves - Top 3 Models')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Interpretability\n",
    "\n",
    "**Warning**: Do not change anything after viewing SHAP results. If changes are needed, rerun the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Select top 2-3 performers for SHAP analysis\n",
    "\n",
    "# Select top 3 (accounting for potential winner's curse, we analyze multiple)\n",
    "shap_models = []\n",
    "\n",
    "for i, (_, row) in enumerate(top_3.iterrows()):\n",
    "    key = f\"{row['model']}_{row['resampling']}\"\n",
    "    if key in final_models:\n",
    "        shap_models.append({\n",
    "            'name': key,\n",
    "            'model': final_models[key]['model'],\n",
    "            'threshold': final_models[key]['threshold']\n",
    "        })\n",
    "\n",
    "print(f\"Models selected for SHAP analysis: {[m['name'] for m in shap_models]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Global SHAP - Bar and Beeswarm plots\n",
    "\n",
    "for model_info in shap_models:\n",
    "    model_name = model_info['name']\n",
    "    model = model_info['model']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SHAP Analysis: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    if 'LightGBM' in model_name or 'XGBoost' in model_name or 'RandomForest' in model_name:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "    else:\n",
    "        # For Logistic Regression, use a sample for background\n",
    "        background = shap.sample(X_train_final, 100, random_state=42)\n",
    "        explainer = shap.LinearExplainer(model, background)\n",
    "    \n",
    "    # Calculate SHAP values on test set\n",
    "    shap_values = explainer.shap_values(X_test_final)\n",
    "    \n",
    "    # Handle different SHAP output formats\n",
    "    if isinstance(shap_values, list):\n",
    "        # For classifiers that return [class_0, class_1]\n",
    "        shap_values_display = shap_values[1]  # Use positive class\n",
    "    else:\n",
    "        shap_values_display = shap_values\n",
    "    \n",
    "    # Bar plot (global feature importance)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_display, X_test_final, plot_type='bar', \n",
    "                      max_display=20, show=False)\n",
    "    plt.title(f'Global Feature Importance - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Beeswarm plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_display, X_test_final, max_display=20, show=False)\n",
    "    plt.title(f'SHAP Beeswarm - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Store for later use\n",
    "    model_info['explainer'] = explainer\n",
    "    model_info['shap_values'] = shap_values_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Local SHAP - Waterfall and Force plots for example cases\n",
    "\n",
    "# Use first model for local explanations\n",
    "model_info = shap_models[0]\n",
    "model_name = model_info['name']\n",
    "model = model_info['model']\n",
    "threshold = model_info['threshold']\n",
    "shap_values_display = model_info['shap_values']\n",
    "\n",
    "print(f\"\\nLocal explanations for: {model_name}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions\n",
    "test_proba = model.predict_proba(X_test_final)[:, 1]\n",
    "test_pred = (test_proba >= threshold).astype(int)\n",
    "\n",
    "# Find example cases\n",
    "tp_idx = np.where((test_pred == 1) & (y_test.values == 1))[0]\n",
    "fn_idx = np.where((test_pred == 0) & (y_test.values == 1))[0]\n",
    "fp_idx = np.where((test_pred == 1) & (y_test.values == 0))[0]\n",
    "\n",
    "print(f\"True Positives: {len(tp_idx)}\")\n",
    "print(f\"False Negatives: {len(fn_idx)}\")\n",
    "print(f\"False Positives: {len(fp_idx)}\")\n",
    "\n",
    "# Create SHAP Explanation object for waterfall plots\n",
    "shap_exp = shap.Explanation(\n",
    "    values=shap_values_display,\n",
    "    base_values=model_info['explainer'].expected_value if not isinstance(model_info['explainer'].expected_value, list) \n",
    "                else model_info['explainer'].expected_value[1],\n",
    "    data=X_test_final.values,\n",
    "    feature_names=X_test_final.columns.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25b: Waterfall plots for TP, FN, FP examples\n",
    "\n",
    "# True Positive example\n",
    "if len(tp_idx) > 0:\n",
    "    idx = tp_idx[0]\n",
    "    print(f\"\\nTrue Positive Example (index {idx}):\")\n",
    "    print(f\"  Predicted probability: {test_proba[idx]:.3f}\")\n",
    "    print(f\"  Actual: Fail\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(shap_exp[idx], max_display=15, show=False)\n",
    "    plt.title(f'True Positive - Sample {idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# False Negative example\n",
    "if len(fn_idx) > 0:\n",
    "    idx = fn_idx[0]\n",
    "    print(f\"\\nFalse Negative Example (index {idx}):\")\n",
    "    print(f\"  Predicted probability: {test_proba[idx]:.3f}\")\n",
    "    print(f\"  Actual: Fail (missed!)\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(shap_exp[idx], max_display=15, show=False)\n",
    "    plt.title(f'False Negative - Sample {idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# False Positive example\n",
    "if len(fp_idx) > 0:\n",
    "    idx = fp_idx[0]\n",
    "    print(f\"\\nFalse Positive Example (index {idx}):\")\n",
    "    print(f\"  Predicted probability: {test_proba[idx]:.3f}\")\n",
    "    print(f\"  Actual: Pass (false alarm)\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(shap_exp[idx], max_display=15, show=False)\n",
    "    plt.title(f'False Positive - Sample {idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Force plots for the same examples\n",
    "\n",
    "# Initialize JS visualization\n",
    "shap.initjs()\n",
    "\n",
    "# Force plot for TP\n",
    "if len(tp_idx) > 0:\n",
    "    idx = tp_idx[0]\n",
    "    print(f\"\\nForce Plot - True Positive (index {idx}):\")\n",
    "    display(shap.force_plot(\n",
    "        shap_exp.base_values[idx] if hasattr(shap_exp.base_values, '__len__') and len(shap_exp.base_values) > 1 else shap_exp.base_values,\n",
    "        shap_exp.values[idx],\n",
    "        X_test_final.iloc[idx],\n",
    "        feature_names=X_test_final.columns.tolist()\n",
    "    ))\n",
    "\n",
    "# Force plot for FN\n",
    "if len(fn_idx) > 0:\n",
    "    idx = fn_idx[0]\n",
    "    print(f\"\\nForce Plot - False Negative (index {idx}):\")\n",
    "    display(shap.force_plot(\n",
    "        shap_exp.base_values[idx] if hasattr(shap_exp.base_values, '__len__') and len(shap_exp.base_values) > 1 else shap_exp.base_values,\n",
    "        shap_exp.values[idx],\n",
    "        X_test_final.iloc[idx],\n",
    "        feature_names=X_test_final.columns.tolist()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27: Feature clustering (correlated features grouped)\n",
    "\n",
    "model_info = shap_models[0]\n",
    "shap_values_display = model_info['shap_values']\n",
    "\n",
    "print(f\"\\nFeature Clustering Analysis for: {model_info['name']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hierarchical clustering of features based on SHAP values\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create clustering plot\n",
    "clustering = shap.utils.hclust(X_test_final, y_test)\n",
    "\n",
    "shap.plots.bar(\n",
    "    shap.Explanation(\n",
    "        values=shap_values_display,\n",
    "        base_values=model_info['explainer'].expected_value if not isinstance(model_info['explainer'].expected_value, list) \n",
    "                    else model_info['explainer'].expected_value[1],\n",
    "        data=X_test_final.values,\n",
    "        feature_names=X_test_final.columns.tolist()\n",
    "    ),\n",
    "    max_display=20,\n",
    "    clustering=clustering,\n",
    "    clustering_cutoff=0.5,\n",
    "    show=False\n",
    ")\n",
    "plt.title(f'Feature Importance with Clustering - {model_info[\"name\"]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 28: Final summary\n\nprint(\"=\"*80)\nprint(\"FINAL ANALYSIS SUMMARY\")\nprint(\"=\"*80)\n\nprint(\"\\n1. DATA OVERVIEW:\")\nprint(f\"   - Original features: {feature_engineering_log['original_features']}\")\nprint(f\"   - Final features after engineering: {len(final_feature_cols_with_indicators)}\")\nprint(f\"   - Training samples: {len(y_train)}\")\nprint(f\"   - Test samples: {len(y_test)}\")\nprint(f\"   - Defect rate: {y_train.mean():.2%}\")\n\nprint(\"\\n2. BEST MODEL CONFIGURATION:\")\nbest_result = test_results_df.iloc[0]\nprint(f\"   - Model: {best_result['model']}\")\nprint(f\"   - Resampling: {best_result['resampling']}\")\nprint(f\"   - Threshold (from CV): {best_result['threshold']:.3f}\")\n\nprint(\"\\n3. TEST SET PERFORMANCE (using fixed threshold from CV):\")\nprint(f\"   - F1 Score: {best_result['f1']:.4f}\")\nprint(f\"   - PR-AUC: {best_result['pr_auc']:.4f}\")\nprint(f\"   - Recall: {best_result['recall']:.4f}\")\nprint(f\"   - Precision: {best_result['precision']:.4f}\")\n\nprint(\"\\n4. KEY INSIGHTS FROM SHAP:\")\nif shap_models:\n    # Get top features\n    mean_abs_shap = np.abs(shap_models[0]['shap_values']).mean(axis=0)\n    top_features_idx = np.argsort(mean_abs_shap)[-5:][::-1]\n    top_features = [X_test_final.columns[i] for i in top_features_idx]\n    print(f\"   Top 5 most important features:\")\n    for i, feat in enumerate(top_features, 1):\n        print(f\"   {i}. {feat}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Analysis complete!\")\nprint(\"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}